{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport glob\nimport sys\nimport random\nimport tqdm as tqdm\n\nimport matplotlib.pyplot as plt\n\nfrom skimage.io import imread, imshow\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\n\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Input, BatchNormalization, merge, GlobalMaxPooling2D, Lambda\nfrom keras.layers.core import Flatten, Dense, Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.applications.vgg19 import VGG19\n\nimport tensorflow as tf\n\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n\nfrom scipy.stats import norm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e9411e7fa24e64160e7e8e208575c2a32c924cd"
      },
      "cell_type": "code",
      "source": "#Going through the images one by one, we found some of the images that are not good for training, due to bad quality, photo taken too far away, bad image angle or simply too noisy. these images are listed below.\nbad_image=['0ae69f1e.jpg', '0b1e39ff.jpg', '0d614410.jpg', '0fb4c4dd.jpg', '01c11eaf.jpg', '1ae163da.jpg', '2a2ecd4b.jpg', '2bc459eb.jpg', '2c824757.jpg', '2f1b1a58.jpg', '3e793658.jpg', '3f919786.jpg', '4d818204.jpg',\n           '05d402e7.jpg', 'obde3564.jpg', '5efe6139.jpg', '06ffe77b.jpg', '6b9f5632.jpg', '6bc32ac5.jpg', '7a399627.jpg', '7aadcef5.jpg', '7cc11b57.jpg', '7d008e04.jpg', '7eacaef2.jpg', '7f048f21.jpg', '7f7702dc.jpg', \n           '8aafd575.jpg', '8b615df8.jpg', '8bd96828.jpg', '8eb500b3.jpg', '9e6ff81f.jpg', '19b2000c.jpg', '29ab7864.jpg', '35e0706e.jpg', '53ccd15c.jpg', '54aeef4a.jpg', '55cb38a4.jpg', '56fafc52.jpg', '69d946a0.jpg', \n           '68ffd7bf.jpg', '73cfb77f.jpg', '342ea7cf.jpg', '383eb5a3.jpg', '734d181a.jpg', '761b8f76.jpg', '806cf583.jpg', '851a3114.jpg', '932d8f1e.jpg', '993f0479.jpg', '1323a889.jpg', '3401bafe.jpg', '14944e45.jpg', \n           '20594d9d.jpg', '28405ee2.jpg', '34333c52.jpg', '43189ff2.jpg', '91885aec.jpg', '300806a1.jpg', '666282d2.jpg', '1990152d.jpg', '42025982.jpg', '95226283.jpg', 'a3e9070d.jpg', 'a3844c28.jpg', 'a2095252.jpg', \n           'ade8176b.jpg', 'ae2f76dc.jpg', 'b1cfda8a.jpg', 'b006cec6.jpg', 'b6e4f09a.jpg', 'b985ae1e.jpg', 'b9315c19.jpg', 'bc5bf694.jpg', 'c8d44ff3.jpg', 'c97c3ae6.jpg', 'c482d51b.jpg', 'c912ac66.jpg', 'c6854c76.jpg', \n           'cc8eb5e2.jpg', 'ced4a25c.jpg', 'cf756424.jpg', 'd14f0126.jpg', 'd79047b6.jpg', 'd781262d.jpg', 'd1502267.jpg', 'dc0e29fe.jpg', 'dc79e75f.jpg', 'de5e2ea7.jpg', 'de8d631f.jpg', 'e0b00a14.jpg', 'e6ce415f.jpg', \n           'e9bd2e9c.jpg', 'e30d9525.jpg', 'e53d2b96.jpg', 'eafadfb3.jpg', 'ee897d4a.jpg', 'f00f98fb.jpg', 'f1b24b92.jpg', 'f4063698.jpg', 'fb2271f1.jpg']\n\nprint (f'Total of {len(bad_image)} images are not suitable for training')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total of 100 images are not suitable for training\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Define train and test path\nTRAIN_PATH=\"../input/whale-categorization-playground/train/\"\nTEST_PATH=\"../input/whale-categorization-playground/test/\"\n\n#read train file summary, minus bad files\ndf=pd.read_csv('../input/whale-categorization-playground/train.csv')\ndf=df[~df['Image'].isin(bad_image)]\n\n#set 20% of image for validation purpose\nnp.random.seed(100)\ndf['train']=np.random.randint(0,10, len(df))>1\n\nprint (f'Total train files = {len(df)}')\n\n#group the train files by ID\nID_freq=df['Id'].value_counts()\nprint (f'Total unique ID= {len(ID_freq)}')\n\nID_count_by_freq=ID_freq.value_counts()\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(ID_count_by_freq)+1), list(ID_count_by_freq))\nplt.xticks(np.arange(1, len(ID_count_by_freq)+1), list(ID_freq.unique()[::-1]))\nplt.xlabel('number of images')\nplt.ylabel('count')\nplt.show()\n\nprint (ID_freq.head())\n#find out how many percent of train file consist of new whale\npercent_new_whale= ID_freq.loc['new_whale']/len(df)\nprint (f'\\n{percent_new_whale*100}% of images are new whale')",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total train files = 9752\nTotal unique ID= 4236\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7f705a56f978>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAF3CAYAAAAcmcfdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOG2qAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "new_whale    805\nw_1287fbc     34\nw_98baff9     27\nw_7554f44     24\nw_693c9ee     22\nName: Id, dtype: int64\n\n8.254716981132075% of images are new whale\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ebc09ab0c186eb507bdcd9baff2e2a2cfb2bde5f"
      },
      "cell_type": "markdown",
      "source": "Majority of ID have only one, two or three images.\nabout 10% of images (after removing bad images) are new whale."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad7787f9da2146e7b967db9b7cd653cd5eadfade",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"Uncomment to train on train files only\"\n#df_train=df[df['train']==True]\n\"Uncomment to train on all files\"\ndf_train=df.copy()\n\nID_freq=df_train['Id'].value_counts()",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ead5bd5c8c20ee9e29d56120cced251bdd80214d"
      },
      "cell_type": "markdown",
      "source": "\nI also created a simple neural network that localize the whale fluke from the image. This model return upper left and bottom right corner of the bounding box for the fluke. We use the outcome to crop the image before feeding them for training.\n\nCredit to @lisa needs braces for providing training set. (https://www.kaggle.com/c/whale-categorization-playground/discussion/57108)\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e14b10cec9ea29eeaf8c3221db55f18efb597352"
      },
      "cell_type": "code",
      "source": "train_crop_table=pd.read_csv('../input/crop-coordinate/train_crop_coordinate.csv', index_col=0)\ntest_crop_table=pd.read_csv('../input/crop-coordinate/test_crop_coordinate.csv', index_col=0)\n\n\"\"\"\nIn this table:\n\"image\"                     -- image name\n\"ori height\"                -- original height of the image\n\"ori width                  -- original width of the image\n\"ystart, xstart\"            -- the upper left corner of bounding box of the image in 224x224 size\n\"yend, xend\"                -- the bottom right corner of bounding box of the image in 224x224 size\n\"new ystart, new xstart\"    -- the upper left corner of bounding box of the image in original size\n\"new yend, new xend\"        -- the bottom right corner of bounding box of the image in original size\n\"\"\"\n\ntrain_crop_table.head()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "          image  ori height  ori width  ystart  xstart  yend  xend  \\\n0  8ab4077d.jpg         158        300      30      10   189   211   \n1  f32f6809.jpg         591       1050      68      36   149   188   \n2  76caefe3.jpg         503       1050       1       4   198   208   \n3  e39b55ef.jpg         450       1050      17       1   149   218   \n4  29910dbd.jpg         430       1050       9       2   185   211   \n\n   new ystart  new xstart  new yend  new xend  \n0          21          13       133       283  \n1         179         169       393       881  \n2           2          19       445       975  \n3          34           5       299      1022  \n4          17           9       355       989  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>ori height</th>\n      <th>ori width</th>\n      <th>ystart</th>\n      <th>xstart</th>\n      <th>yend</th>\n      <th>xend</th>\n      <th>new ystart</th>\n      <th>new xstart</th>\n      <th>new yend</th>\n      <th>new xend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8ab4077d.jpg</td>\n      <td>158</td>\n      <td>300</td>\n      <td>30</td>\n      <td>10</td>\n      <td>189</td>\n      <td>211</td>\n      <td>21</td>\n      <td>13</td>\n      <td>133</td>\n      <td>283</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f32f6809.jpg</td>\n      <td>591</td>\n      <td>1050</td>\n      <td>68</td>\n      <td>36</td>\n      <td>149</td>\n      <td>188</td>\n      <td>179</td>\n      <td>169</td>\n      <td>393</td>\n      <td>881</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>76caefe3.jpg</td>\n      <td>503</td>\n      <td>1050</td>\n      <td>1</td>\n      <td>4</td>\n      <td>198</td>\n      <td>208</td>\n      <td>2</td>\n      <td>19</td>\n      <td>445</td>\n      <td>975</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e39b55ef.jpg</td>\n      <td>450</td>\n      <td>1050</td>\n      <td>17</td>\n      <td>1</td>\n      <td>149</td>\n      <td>218</td>\n      <td>34</td>\n      <td>5</td>\n      <td>299</td>\n      <td>1022</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>29910dbd.jpg</td>\n      <td>430</td>\n      <td>1050</td>\n      <td>9</td>\n      <td>2</td>\n      <td>185</td>\n      <td>211</td>\n      <td>17</td>\n      <td>9</td>\n      <td>355</td>\n      <td>989</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "850e675779ec3a2e8d4d3eb3c77cc610ac9a81b8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "IMG_SHAPE=(224, 224, 3)\nOUTPUT_DIMENSION=800\n#define base model for croping and triplet loss\ndef get_base_model():\n    \"\"\" load vgg19 model with no top, add final output as dense layer with dimension OUTPUT_DIMENSION\n    \"\"\"\n    \n    base_model=VGG19(include_top=False, weights=None)\n    #base_model.load_weights('../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', by_name=True)\n    base_model.load_weights('../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', by_name=True)\n    \n    x=base_model.output\n    x=GlobalMaxPooling2D()(x)\n    x=Dropout(0.3)(x)\n    dense_1=Dense(OUTPUT_DIMENSION)(x)\n    normalized=Lambda(lambda x: K.l2_normalize(x, axis=1))(dense_1)\n    base_model=Model(base_model.input, normalized)\n    return base_model\n\ndef get_triplet_model(lr=0.001):\n    input1=Input(IMG_SHAPE)\n    input2=Input(IMG_SHAPE)\n    input3=Input(IMG_SHAPE)\n    \n    output1=base_model(input1)\n    output2=base_model(input2)\n    output3=base_model(input3)\n    \n    loss=merge([output1, output2, output3], mode=triplet_loss, output_shape=(1,))\n    \n    model=Model(inputs=[input1, input2, input3], outputs=loss)\n    model.compile(loss=identity_loss, optimizer=Adam(lr))\n    return model\n\ndef triplet_loss(X, alpha=0.2):\n    pos, neg, anchor=X\n    pos_dist=tf.reduce_sum(tf.square(tf.subtract(anchor, pos)), axis=-1)\n    neg_dist=tf.reduce_sum(tf.square(tf.subtract(anchor, neg)), axis=-1)\n    basic_loss=tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n    \n    loss=tf.reduce_mean(tf.maximum(basic_loss, 0))\n    return loss\n\ndef identity_loss(y_true, y_pred):\n    return K.mean(y_pred-0*y_true)\n\ndef check_id(image):\n    ID=df['Id'][df['Image']==image].iloc[0]\n    return ID\n\ndef crop_image(images, train=True):\n    \"\"\"take in images, crop according to train/test crop table, resize to IMG_SHAPE\n    \n    Arguments:\n    images-- list consist of image name to be cropped\n    train-- bool, True if the images are from train, False if test\n    \n    return:\n    output_array-- array with axis-0 as number of images, axis-1 to 3 as cropped array of the input images\n    \"\"\"\n    output_array=np.zeros((len(images), IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2]))\n    for n, image in enumerate (images):\n        if train:\n            path=TRAIN_PATH+image\n            table=train_crop_table.copy()\n        else:\n            path=TEST_PATH+image\n            table=test_crop_table.copy()\n            \n        ystart=table['new ystart'][table['image']==image].values\n        xstart=table['new xstart'][table['image']==image].values\n        yend=table['new yend'][table['image']==image].values\n        xend=table['new xend'][table['image']==image].values\n        \n        img_array=imread(path)\n        height, width=img_array.shape[0], img_array.shape[1]\n        \n        #provide 10 pixel of margin, so that we are sure that entire flute will be cropped.\n        ystart=int(max(ystart-10, 0))\n        xstart=int(max(xstart-10, 0))\n        yend=int(min(yend+10, height))\n        xend=int(min(xend+10, width))\n        \n        #if image is greyscale, convert it to rgb with all channel = greyscale channel\n        if img_array.ndim==3:\n            img_array=img_array[ystart:yend, xstart:xend, :]\n            img_array=resize(img_array, IMG_SHAPE)\n        else:\n            img_array=img_array[ystart:yend, xstart:xend]\n            img_array=resize(img_array, (IMG_SHAPE[0], IMG_SHAPE[1]))\n            img_array=np.expand_dims(img_array, axis=-1)\n            img_array[:,:,:]=img_array\n        output_array[n]=img_array\n    return output_array\n\ndef generate_ID_map(unique_ID):\n    \"\"\" \n    generate a dictionary to map ID in unique_ID to its corresponding images, distances and indices among the images of the same ID\n    \"\"\"\n    global ID_map\n    \n    for ID in unique_ID:\n        img_lst=list(df_train['Image'][df_train['Id']==ID])\n        img_array=crop_image(img_lst)\n        encoding=base_model.predict(img_array)\n        nbrs=NearestNeighbors(n_neighbors=len(img_lst), algorithm= 'ball_tree').fit(encoding)\n        dist, indices=nbrs.kneighbors(encoding)\n        centroid=find_centroid(encoding)\n        \n        ID_map[ID]={'images':img_lst,\n                    'distances': dist, \n                    'indices': indices,\n                    'centroid': centroid}\n    \n    return ID_map\n\ndef find_centroid(encoding):\n    return np.mean(encoding, axis=0)\n\ndef generate_global(img_array):\n    global global_dist, global_idx\n    img_encoding=base_model.predict(img_array)\n    nbrs=NearestNeighbors(n_neighbors=anchor_encoding.shape[0], algorithm= 'ball_tree').fit(anchor_encoding)\n    global_dist, global_idx=nbrs.kneighbors(anchor_encoding)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1955fd0816432b3b857397116f5b7fcdf9b76332",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "base_model=get_base_model()\n\ninput1=Input(IMG_SHAPE)\ninput2=Input(IMG_SHAPE)\ninput3=Input(IMG_SHAPE)\n\noutput1=base_model(input1)\noutput2=base_model(input2)\noutput3=base_model(input3)\n\nloss=merge([output1, output2, output3], mode=triplet_loss, output_shape=(1,))\n\ntriplet_model=Model(inputs=[input1, input2, input3], outputs=loss)\ntriplet_model.compile(loss=identity_loss, optimizer=Adam(10**-5))\n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e9ae2ea52d1f74c15469a2f737009d4fadf2c84",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#Select all ID which has at least 2 images, at most 100 (to eliminate \"new_whale\") for training\nID_SL=list(ID_freq[(ID_freq>=2) & (ID_freq<100)].index)\nprint ('Generating ID_map...')\nID_map={}\ngenerate_ID_map(ID_SL)\nprint (\"ID_map generated\")\nprint (f'\\ntotal number of IDs to train= {len(ID_SL)}')",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Generating ID_map...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e45f02b7b4624d5790d6eeb14461aaf4203ae48",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"Skip this segment if not loading pretrained weight\"\nbase_model.load_weights('../input/128-min2-max4-crop-e2/min2_max4_crop_e2_128.h5', by_name=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "799c11919c3728657d2cbb6f2d213908363edc0c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"Skip this segment if not training.\"\n\nclass data_generator:\n    def __init__(self, ID_map, ID_lst, batch_size, num_img):\n        \n        self.ID_lst=ID_lst\n        self.batch_size=batch_size\n        self.num_img=num_img\n        \n    \n    def find_neighbors(self):\n        self.centroid_array=np.zeros((len(ID_map.keys()), OUTPUT_DIMENSION))\n        \n        for n, ID in enumerate(ID_map.keys()):\n            self.centroid_array[n]=ID_map[ID]['centroid']\n                    \n        centroid_neighbors=NearestNeighbors(n_neighbors=len(self.ID_lst), algorithm='ball_tree').fit(self.centroid_array)\n        _, self.nearest_neighbors=centroid_neighbors.kneighbors(self.centroid_array)\n        \n        return self.centroid_array\n      \n    def generate_positive(self, img_shortlist):\n        self.pos_sample_lst=[]\n        for n, image in enumerate(img_shortlist):\n            ID=check_id(image)\n            img_lst=ID_map[ID]['images']\n            idx=img_lst.index(image)\n            furthest_pos_idx=ID_map[ID]['indices'][idx, len(img_lst)-1]\n            furthest_pos_img=ID_map[ID]['images'][furthest_pos_idx]\n            self.pos_sample_lst.append(furthest_pos_img)\n        return self.pos_sample_lst\n    \n    def generate_negative(self, img_shortlist, diff=1):\n        self.neg_sample_lst=[]\n        for n, image in enumerate(img_shortlist):\n            ID=check_id(image)\n            idx=self.ID_lst.index(ID)\n            #print (self.nearest_neighbors.shape)\n            nearest_centroid=self.ID_lst[self.nearest_neighbors[idx, diff]]#diff= 1 to number of ID group, one being the hardest\n            nearest_nbrs=random.choice(ID_map[nearest_centroid]['images'])\n            self.neg_sample_lst.append(nearest_nbrs)\n            #print (self.nearest_neighbors)\n        return self.neg_sample_lst\n    \n    def generate_training_ID_SL(self):\n        random.shuffle(self.ID_lst)\n        self.img_shortlist=[]\n        for ID in ID_SL:\n            images=ID_map[ID]['images']\n            self.img_shortlist.extend(images)                \n\n    def generate_training_triplet(self):\n        while True:\n            self.generate_training_ID_SL()\n            m=len(self.img_shortlist)\n            batches=int(m/self.batch_size)\n            indexes=np.arange(batches)\n            ID_map=generate_ID_map(ID_SL)\n            self.find_neighbors()\n            \n            for i in indexes:\n                if i==indexes[-1]:\n                    anchor_lst=self.img_shortlist[i*self.batch_size:]\n                       \n                else:\n                    anchor_lst=self.img_shortlist[i*self.batch_size:(i+1)*self.batch_size]\n                \n                \n                \n                pos_lst=self.generate_positive(anchor_lst)\n                neg_lst=self.generate_negative(anchor_lst)\n                \n                anchor_img=crop_image(anchor_lst)\n                pos_img=crop_image(pos_lst)\n                neg_img=crop_image(neg_lst)\n                \n                yield([pos_img, neg_img, anchor_img], np.zeros(anchor_img.shape[0]))\n                \ntraining_images=[]\nID_SL=list(ID_SL)\nprint (f'Total number of IDs to be trained is {len(ID_SL)}')\nnum_img=1500\nbatch_size=16\nsteps_per_epoch=int(num_img/batch_size)\ntraining_data=data_generator(ID_map, ID_SL, batch_size, num_img)\ntriplet_model.fit_generator(training_data.generate_training_triplet(), steps_per_epoch=steps_per_epoch, epochs=10, workers=2)\nbase_model.save_weights('4th_run 10 epochs')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "8f46e972e34393885b3ef5a83f8f5597bf57f570"
      },
      "cell_type": "markdown",
      "source": "Training for 10 epochs using all training data takes about 1.5 hours.\n\nSome modification to reduce training that have yet to be tested out:\n    1. Use smaller but deeper model as base model. Something like resnet\n    2.Further reduce output dimension. High output dimension leads to slow KNN analysis.\n\nTo identify the whale based on the encoding, the following steps are taken:\n1. Regenerate ID_map, to include all IDs that were not used in training, but not the 'new_whale' ID.\n2. Calculate the distance_to_centroid of each image to its' ID centroid. find the mean and std of the distance_to_centroid among the images with same ID.\n3. Calculate the average of mean and std of distance_to_centroid for ID that has > 3 images. This average will then be used as the mean and std for ID with <=3 images.\n4. generate encoding of test images.\n5. create nearest neighbors model with all train encoding. Then find the 30 nearest neighbor of the test encoding.\n6. from the 30 nearest neighbors, find out their ID and append to ID list.\n7. on each ID, find out their centroid, and the distance of test encoding to the centroid. Generate z-score and probability based on the distance of test encoding to centroid, mean and std distance calculated in step 2.\n8. Add 'new_whale' into the ID list, assigning the probability as 0.1 (percent of new_whale in training images)\n9. Choose the top 5 ID based on probability\n10. generate submission file.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8bb5f4a05d09fb41a7213e0d2352009c5fc85d8f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#step 1 to 3\ntrain_unique_ID=ID_freq.index[1:] #list all ID in training set except 'new_whale'\nID_map={}\ntrain_img_lst=df_train['Image'][df_train['Id']!='new_whale'].tolist()\nfor ID in train_unique_ID:\n    img_lst=list(df_train['Image'][df_train['Id']==ID])\n    img_array=crop_image(img_lst)\n    train_encoding=base_model.predict(img_array)\n    nbrs=NearestNeighbors(n_neighbors=len(img_lst), algorithm= 'ball_tree').fit(train_encoding)\n    dist, indices=nbrs.kneighbors(train_encoding)\n    centroid=find_centroid(train_encoding)\n\n    if len(img_lst)<=3:\n        #assign mean and std with average of mean and std from ID>=5\n        mean_dist=0.45 \n        std_dist=0.085\n    else:\n        individual_distance=np.sqrt(np.power(train_encoding-centroid, 2).sum(axis=1))\n        mean_dist=np.mean(individual_distance)\n        std_dist=np.std(individual_distance)\n\n    ID_map[ID]={'images':img_lst,\n                'distances': dist, \n                'indices': indices,\n                'centroid': centroid,\n                'mean dist': mean_dist,\n                'std dist': std_dist}\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "45c83b4c1274c10227f8f64ebccbc32e17c8aa65"
      },
      "cell_type": "markdown",
      "source": "In view of limited RAM, we will load and store train and test encoding batch by batch."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0049da44de0755152522be8041a597fe2f2d9dcf",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#step 4\ndef encoding_image_list(lst, coordinate_table, train=True):\n    encoding=[]\n    for fname, array in predict_data_generator(lst, coordinate_table, batch_size=16, train=train):\n        encoding.extend(base_model.predict(array))\n    return encoding\n\ndef predict_data_generator(lst, coordinate_table, batch_size=16, train=True):\n        m=len(lst)\n        batches=int(m/batch_size)\n        indexes=list(range(batches))\n        for i in indexes:\n            if i==indexes[-1]:\n                fname=lst[i*batch_size:]\n            else:\n                fname=lst[i*batch_size:(i+1)*batch_size] \n            img_array=crop_image(fname, train=train)\n            if i%50==0:\n                print (i, 'of', indexes[-1], 'loaded')\n            yield fname, img_array\n        raise StopIteration()\n\ndef save_train_encoding():\n    print ('loading train encoding list...')\n    train_encoding_lst=encoding_image_list(train_img_lst, train_crop_table)\n    np.save('train_encoding_lst.npy', train_encoding_lst) \n    print ('loading train encoding list completed')\n    return train_encoding_lst\ntrain_encoding_lst=save_train_encoding()\n\n#load all test file name into list\ntest_file_path=glob.glob('../input/whale-categorization-playground/test/*.jpg')\ntest_file_path=test_file_path\ntest_img_lst=[]\nfor file in test_file_path:\n    test_img_lst.append(file.split('/')[-1])\n    \ndef save_test_encoding():\n    print ('loading test encoding list...')\n    test_encoding_lst=encoding_image_list(test_img_lst, test_crop_table, train=False)\n    np.save('test_encoding_lst.npy', test_encoding_lst)\n    print ('loading test encoding list completed')\n    return test_encoding_lst\ntest_encoding_lst=save_test_encoding()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "118cde07575ad32c3d58f7c172df8bfe70e8bafd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#step 5\nn_neighbors=30\nnbrs=NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(train_encoding_lst)\n\nprint ('fitting neighbor model...')\ndist, idx=nbrs.kneighbors(test_encoding_lst)\nprint ('fitting neighbor model completed')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a2e526cbd75c04336a322f0249701cb34345d01",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#step 6-9\ndef choose_top5(test_encoding_lst, NW_cutoff=0.1):\n    top5=[]\n    m=len(test_encoding_lst)\n    for i in range(m):\n        if i%500==0:\n            print ('analyzing ', i , 'of ', m)\n        ID_lst=[]\n        cfm_lst=[]\n\n        for j in idx[i]:\n            ID=check_id(train_img_lst[j])\n            if ID not in ID_lst:\n                ID_lst.append(ID)\n                centroid=ID_map[ID]['centroid']\n                mean_dist=ID_map[ID]['mean dist']\n                std_dist=ID_map[ID]['std dist']\n\n                encoding_dist_to_centroid=np.linalg.norm(test_encoding_lst[i]-centroid)\n                cfm=1-norm.cdf(encoding_dist_to_centroid, mean_dist, std_dist)\n                cfm_lst.append(cfm)\n        \n        if 'new_whale' not in ID_lst:\n            ID_lst.append('new_whale')\n            cfm_lst.append(NW_cutoff) ## NW_cutoff as fraction of train files are new whale.\n        \n        top_5_cfm=np.flip(np.argsort(cfm_lst.copy()), axis=0)\n        ID_lst=np.array(ID_lst)\n        top5.append(list(ID_lst[top_5_cfm[:5]]))\n        \n    return top5\nprint ('predicting top 5 choices')\ntop5=choose_top5(test_encoding_lst)\nprint ('predicting top 5 completed')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ec5eedf9b9031249d3395c74b54a454167909326"
      },
      "cell_type": "code",
      "source": "#step 10\nnew_top5=[]\nfor row in top5:\n    new_row=''\n    for col in row:\n        if col==row[0]:\n            new_row=new_row+''+col\n        else:\n            new_row=new_row+' '+col\n    new_top5.append(new_row)\n        \n\nprint ('writing output file')\nfilename='min2_maxall_128.txt'\nf=open (filename, 'w+')\nf.write('Image,Id\\n')\n\nfor i in range (len(test_img_lst)):\n    data=test_img_lst[i]+','+ new_top5[i]\n    f.write(data+'\\n')\nf.close\nprint ('writing complete')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0250a6b48a9a7d9a92a64c4755841843c1a64666"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}